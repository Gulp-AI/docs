---
title: 'Basic Job Structure'
description: 'Learn how to structure your Gulp Planner YAML configuration files'
---

# Basic Job Structure

Every Gulp Planner job is defined by a YAML file that describes the entire streaming pipeline.

## Core Configuration

```yaml
version: 1.0                # Required: Configuration version
name: "MyStreamingJob"      # Required: Unique job name
parallelism: 4             # Required: Default parallelism
enableObjectReuse: true    # Optional: Enable object reuse for better performance

# Optional: Watermark Configuration for Event Time Processing
watermarkConfig:
  timeCharacteristic: EVENT_TIME  # EVENT_TIME or PROCESSING_TIME
  timestampField: timestamp       # Required for EVENT_TIME
  maxOutOfOrderness: 5000        # Milliseconds

# Optional: Environment Settings
environmentSettings:
  runtimeMode: STREAMING    # STREAMING or BATCH
  maxParallelism: 128      # Maximum parallelism for rescaling
  restartStrategy:
    type: fixed-delay
    attempts: 3
    delay: 10
```

## State Management

```yaml
stateBackend:
  type: rocksdb              # rocksdb, hashmap, memory
  incrementalCheckpoints: true
  storageConfig:
    type: filesystem        # filesystem, s3
    basePath: "s3://..."
    checkpointsPath: "s3://..."
    savepointsPath: "s3://..."
  retainOnCancellation: true
  minPauseBetweenCheckpoints: 1000
  tolerableCheckpointFailureNumber: 2

checkpointing:
  interval: 6000           # Milliseconds
  mode: exactly_once      # exactly_once or at_least_once
  timeout: 60             # Seconds
  maxConcurrentCheckpoints: 1
```

## Sources and Sinks

<Note>
  Sources and sinks are required components in every Gulp Planner job configuration.
</Note>

### Source Configuration
```yaml
sources:
  - id: rawData              # Required: Unique source ID
    name: "Raw Data Source"  # Optional: Descriptive name
    type: kafka             # Required: Source type
    sourceSchema:           # Required: Data schema
      type: "object"
      properties:
        id:
          type: "long"
        value:
          type: "double"
        timestamp:
          type: "long"
        category:
          type: "string"
        metadata:
          type: "object"
          properties:
            region:
              type: "string"
            tags:
              type: "array"
              items:
                type: "string"
```

### Sink Configuration
```yaml
sinks:
  - id: processedData
    name: "Processed Data Sink"
    type: kafka
    input: transformedStream    # ID of source or transformation
```

## Transformations

<Note>
  Transformations are optional components that allow you to process and modify your data stream.
</Note>

```yaml
transformations:
  - id: enrichedStream
    name: "Data Enrichment"
    type: filter              # transformation type
    input: rawData           # source or transformation ID
    filterRule:
      field: value
      operator: "gt"
      value: 0.0
```

## Type Reference

### Basic Types

| Type | Description | Example |
|:-----|:------------|:--------|
| `string` | Text data | `"hello"` |
| `long` | 64-bit integer | `42` |
| `integer` | 32-bit integer | `100` |
| `double` | Double precision float | `3.14` |
| `float` | Single precision float | `3.14` |
| `boolean` | Boolean value | `true` |
| `date` | Date without time | `"2024-01-20"` |
| `timestamp` | Date with time | `"2024-01-20T15:30:00Z"` |

### Complex Types

| Type | Description | Example |
|:-----|:------------|:--------|
| `object` | Nested object | See below |
| `array` | List of values | See below |
| `map` | Key-value pairs | See below |

#### Object Type
```yaml
type: "object"
properties:
  name:
    type: "string"
  age:
    type: "integer"
  address:
    type: "object"
    properties:
      street:
        type: "string"
      city:
        type: "string"
required: ["name", "age"]  # Optional: list of required fields
```

#### Array Type
```yaml
type: "array"
items:
  type: "string"    # Type of array elements
minItems: 1         # Optional: minimum items
maxItems: 10        # Optional: maximum items
```

#### Map Type
```yaml
type: "object"
additionalProperties:
  type: "string"    # Type of map values
```

## Sink Configuration Examples

<Note>
  These are required configurations for specific sink types.
</Note>

### MSK (Amazon Managed Streaming for Kafka)
```yaml
sinks:
  - id: mskSink
    name: "MSK Analytics Output"
    type: kafka
    input: transformedStream
    config:
      # MSK Connection
      bootstrapServers: "${MSK_BOOTSTRAP_SERVERS}"
      topic: "analytics-output"
      securityProtocol: "SASL_SSL"
      saslMechanism: "AWS_MSK_IAM"
      
      # Producer Configuration
      deliveryGuarantee: "exactly_once"  # exactly_once, at_least_once
      transactionTimeoutMs: 900000       # 15 minutes
      batchSize: 16384                   # Size in bytes
      linger.ms: 50                      # Artificial delay for batching
      compression.type: "snappy"         # none, gzip, snappy, lz4, zstd
      
      # Performance Tuning
      maxRequestSize: 1048576           # 1MB
      bufferMemory: 33554432            # 32MB
      maxBlockMs: 60000                 # 1 minute
      retries: 3
      
      # Schema Configuration
      keySerializationSchema:
        type: "string"                  # Key serialization type
        fields: ["category"]            # Fields to use as key
      valueSerializationSchema:
        type: "json"                    # json, avro, protobuf
        includeFields: ["timestamp", "value", "metadata"]
```

### Elasticsearch
```yaml
sinks:
  - id: elasticSink
    name: "Elasticsearch Metrics"
    type: elasticsearch
    input: aggregatedMetrics
    config:
      hosts: ["https://vpc-es-domain.region.es.amazonaws.com"]
      index: "metrics-#{date:YYYY.MM.dd}"  # Dynamic index naming
      documentType: "_doc"
      bulkFlushMaxActions: 1000
      bulkFlushMaxSizeBytes: "5mb"
      bulkFlushInterval: 1000              # milliseconds
      retries: 3
      failureHandler: "retry"              # retry, ignore, fail
      
      # Schema Configuration
      documentIdField: "id"                # Field to use as document ID
      routing:
        field: "category"                  # Elasticsearch routing field
        
      # Connection Settings
      connectionPathPrefix: "/analytics"
      connectionRequestTimeout: 5000
``` 