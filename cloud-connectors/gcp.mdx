---
title: 'GCP Integration'
description: 'Learn how to integrate Gulp Planner with Google Cloud Platform services'
---

# Google Cloud Platform Integration

Gulp Planner provides seamless integration with Google Cloud Platform (GCP) services for both source and sink configurations.

## Google Cloud Pub/Sub

### Pub/Sub Source

<CodeGroup>
```yaml Source Configuration
sources:
  - id: "pubsub-input"
    type: "pubsub"
    config:
      projectId: "my-project"
      subscriptionName: "my-subscription"
      # Optional configurations
      maxOutstandingMessages: 1000
      maxOutstandingBytes: 1000000000
      pullInterval: 100
      credentials:
        type: "service_account"
        path: "/path/to/credentials.json"
        # Or use JSON directly
        json: ${GOOGLE_CREDENTIALS_JSON}
    schema:
      timestamp: "java.lang.Long"
      payload: "java.lang.String"
      attributes:
        user_id: "java.lang.String"
        event_type: "java.lang.String"
```
</CodeGroup>

### Pub/Sub Sink

<CodeGroup>
```yaml Sink Configuration
sinks:
  - id: "pubsub-output"
    type: "pubsub"
    input: "transform-1"
    config:
      projectId: "my-project"
      topicName: "my-topic"
      # Optional configurations
      batchSize: 100
      requestBytesThreshold: 1000000
      delayThreshold: 100
      retrySettings:
        initialRetryDelay: 100
        retryDelayMultiplier: 1.3
        maxRetryDelay: 60000
        initialRpcTimeout: 5000
        maxRpcTimeout: 600000
      credentials:
        type: "service_account"
        path: "/path/to/credentials.json"
```
</CodeGroup>

## Google BigQuery

### BigQuery Source

<CodeGroup>
```yaml Source Configuration
sources:
  - id: "bigquery-input"
    type: "bigquery"
    config:
      projectId: "my-project"
      dataset: "my_dataset"
      table: "my_table"
      # Query-based configuration
      query: "SELECT * FROM `my_project.my_dataset.my_table` WHERE timestamp > @startTime"
      queryParameters:
        startTime:
          type: "TIMESTAMP"
          value: "2024-01-01T00:00:00Z"
      # Optional configurations
      fetchSize: 1000
      credentials:
        type: "service_account"
        path: "/path/to/credentials.json"
    schema:
      id: "java.lang.String"
      timestamp: "java.lang.Long"
      value: "java.lang.Double"
```
</CodeGroup>

### BigQuery Sink

<CodeGroup>
```yaml Sink Configuration
sinks:
  - id: "bigquery-output"
    type: "bigquery"
    input: "transform-1"
    config:
      projectId: "my-project"
      dataset: "my_dataset"
      table: "my_table"
      # Optional configurations
      createDisposition: "CREATE_IF_NEEDED"
      writeDisposition: "WRITE_APPEND"
      batchSize: 1000
      retrySettings:
        maxAttempts: 3
        initialRetryDelay: 1000
      credentials:
        type: "service_account"
        path: "/path/to/credentials.json"
```
</CodeGroup>

## Google Cloud Storage

### Cloud Storage Source

<CodeGroup>
```yaml Source Configuration
sources:
  - id: "gcs-input"
    type: "gcs"
    config:
      bucket: "my-bucket"
      path: "input/data/"
      # File format configuration
      format: "parquet"  # json, csv, parquet, avro
      # Optional configurations
      watchInterval: 60000  # Milliseconds
      filePattern: "*.parquet"
      credentials:
        type: "service_account"
        path: "/path/to/credentials.json"
    schema:
      timestamp: "java.lang.Long"
      value: "java.lang.Double"
```
</CodeGroup>

### Cloud Storage Sink

<CodeGroup>
```yaml Sink Configuration
sinks:
  - id: "gcs-output"
    type: "gcs"
    input: "transform-1"
    config:
      bucket: "my-bucket"
      path: "output/data/"
      # File format configuration
      format: "parquet"  # json, csv, parquet, avro
      # Optional configurations
      partitionFields: ["year", "month", "day"]
      rolloverSize: 128000000  # Bytes
      rolloverInterval: 60000  # Milliseconds
      compression: "gzip"  # none, gzip, snappy
      credentials:
        type: "service_account"
        path: "/path/to/credentials.json"
```
</CodeGroup>

## Best Practices

<AccordionGroup>
  <Accordion title="Authentication">
    - Use service accounts with minimal required permissions
    - Rotate service account keys regularly
    - Store credentials securely using secret management solutions
    - Use Workload Identity when running in GKE
  </Accordion>

  <Accordion title="Performance Optimization">
    - Configure appropriate batch sizes for Pub/Sub and BigQuery
    - Use partitioning in BigQuery for better query performance
    - Enable compression for Cloud Storage sinks
    - Monitor and adjust buffer sizes based on throughput
  </Accordion>

  <Accordion title="Cost Optimization">
    - Use streaming inserts for BigQuery only when necessary
    - Configure appropriate retention policies for Pub/Sub
    - Use Cloud Storage lifecycle policies
    - Monitor data transfer and API usage costs
  </Accordion>

  <Accordion title="Error Handling">
    - Configure retry policies for transient failures
    - Use dead letter topics in Pub/Sub
    - Monitor quota usage and limits
    - Implement appropriate error handling for BigQuery streaming errors
  </Accordion>

  <Accordion title="Monitoring">
    - Set up Cloud Monitoring metrics and alerts
    - Monitor Pub/Sub subscription backlog
    - Track BigQuery slot utilization
    - Monitor Cloud Storage object lifecycle
  </Accordion>

  <Accordion title="Schema Evolution">
    - Use schema evolution features in BigQuery
    - Plan for backward compatibility
    - Test schema changes before deployment
    - Document schema versioning strategy
  </Accordion>
</AccordionGroup> 