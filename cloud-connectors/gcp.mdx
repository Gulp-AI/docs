---
title: "GCP Integration"
description: "Configure GCP sources and sinks in Gulp"
---

# GCP Integration

Gulp provides native integration with Google Cloud Platform (GCP) services for both sources and sinks. When running in GCP environments, authentication is handled automatically through service accounts.

## Sources

### Google Cloud Storage (GCS)

```yaml
source:
  type: "gcs"
  config:
    bucket: "my-data-bucket"
    path: "input/data/*.parquet"
    format: "parquet"  # parquet, json, csv, avro
    maxConcurrentRequests: 10
    monitoring:
      enabled: true
      metrics: ["latency", "throughput", "errors"]
    checkpointing:
      enabled: true
      interval: "1m"
```

### Google Cloud Pub/Sub

```yaml
source:
  type: "pubsub"
  config:
    subscription: "projects/my-project/subscriptions/my-subscription"
    maxOutstandingMessages: 1000
    maxOutstandingBytes: "1gb"
    monitoring:
      enabled: true
      metrics: ["latency", "throughput", "errors"]
    watermark:
      timestampField: "event_time"
      maxOutOfOrderness: "5s"
```

### BigQuery

```yaml
source:
  type: "bigquery"
  config:
    project: "my-project"
    dataset: "my_dataset"
    table: "my_table"
    viewMaterializationProject: "my-project"  # For views
    viewMaterializationDataset: "temp_dataset"
    parallelism: 10
    monitoring:
      enabled: true
      metrics: ["latency", "throughput", "errors"]
```

### Cloud Spanner

```yaml
source:
  type: "spanner"
  config:
    instance: "my-instance"
    database: "my-database"
    table: "my-table"
    project: "my-project"
    parallelism: 10
    monitoring:
      enabled: true
      metrics: ["latency", "throughput", "errors"]
```

## Sinks

### Google Cloud Storage (GCS)

```yaml
sink:
  type: "gcs"
  config:
    bucket: "my-output-bucket"
    path: "output/data"
    format: "parquet"
    partitioning:
      - field: "year"
        extractFromTimestamp: "event_time"
      - field: "month"
        extractFromTimestamp: "event_time"
    compression: "snappy"
    maxFileSize: "128mb"
    inactivityInterval: "5m"
```

### Google Cloud Pub/Sub

```yaml
sink:
  type: "pubsub"
  config:
    topic: "projects/my-project/topics/my-topic"
    batchSize: 100
    maxBatchSize: "1mb"
    maxBatchDuration: "500ms"
    orderingKey: "user_id"  # For ordered delivery
    monitoring:
      enabled: true
      metrics: ["latency", "throughput", "errors"]
```

### BigQuery

```yaml
sink:
  type: "bigquery"
  config:
    project: "my-project"
    dataset: "my_dataset"
    table: "my_table"
    createDisposition: "CREATE_IF_NEEDED"
    writeDisposition: "WRITE_APPEND"
    partitioning:
      type: "time"
      field: "event_time"
      expirationMs: 2592000000  # 30 days
    clustering:
      fields: ["user_id", "event_type"]
    monitoring:
      enabled: true
      metrics: ["latency", "throughput", "errors"]
```

### Cloud Spanner

```yaml
sink:
  type: "spanner"
  config:
    instance: "my-instance"
    database: "my-database"
    table: "my-table"
    project: "my-project"
    batchSize: 1000
    commitDeadline: "10s"
    maxCommitDelay: "500ms"
    monitoring:
      enabled: true
      metrics: ["latency", "throughput", "errors"]
```

## Authentication

When running in GCP environments, authentication is handled automatically through service accounts. No explicit credentials need to be specified in the YAML configuration.

### Required IAM Roles

Each service requires specific IAM roles. Here are the common ones:

<CodeGroup>
```yaml Cloud Storage
roles:
  - roles/storage.objectViewer     # For sources
  - roles/storage.objectCreator    # For sinks
```

```yaml Pub/Sub
roles:
  - roles/pubsub.subscriber       # For sources
  - roles/pubsub.publisher       # For sinks
```

```yaml BigQuery
roles:
  - roles/bigquery.dataViewer    # For sources
  - roles/bigquery.dataEditor    # For sinks
```

```yaml Cloud Spanner
roles:
  - roles/spanner.databaseReader  # For sources
  - roles/spanner.databaseUser    # For sinks
```
</CodeGroup>

## Best Practices

1. **Service Accounts**: Use dedicated service accounts with minimal permissions
2. **VPC Service Controls**: Implement for enhanced security
3. **Batching**: Configure appropriate batch sizes for optimal performance
4. **Monitoring**: Enable Cloud Monitoring integration for all connectors
5. **Cost Optimization**: Use appropriate partitioning and clustering

## Error Handling

All GCP connectors support consistent error handling configuration:

```yaml
errorHandling:
  retries:
    maxAttempts: 3
    backoffPeriod: "1s"
    maxBackoffPeriod: "1m"
    multiplier: 2.0
  deadLetter:
    enabled: true
    destination: "gs://my-bucket/dead-letter"
```

## Monitoring

Enable detailed monitoring with Cloud Monitoring integration:

```yaml
monitoring:
  metrics:
    - name: "throughput"
      interval: "1m"
    - name: "latency"
      interval: "1m"
    - name: "errors"
      interval: "1m"
  alerts:
    - name: "HighLatency"
      threshold: "5s"
      duration: "5m"
    - name: "HighErrorRate"
      threshold: 0.01
      duration: "5m"
```

<Note>
For production deployments, it's recommended to enable monitoring and configure appropriate alerts.
</Note> 